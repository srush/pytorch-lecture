{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "> \"Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that a simple mathematical function (the cosine similarity between the vectors) indicates the level of semantic similarity between the words represented by those vectors.\" [ https://en.wikipedia.org/wiki/Word2vec ]\n",
    "\n",
    "\n",
    "There are two Word2Vec architectures: \n",
    "\n",
    "- **CBOW (Continuous Bag-of-Words)** predicts the central word from the sum of context vectors. This simple sum of word vectors is called \"bag of words\", which gives the name for the model.\n",
    "\n",
    "- **Skip-Gram** predicts context words given the central word. Skip-Gram with negative sampling is the most popular approach.\n",
    "\n",
    "Here we will build a PyTorch model that implements Word2Vec's CBOW strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Daniel_Braun6/publication/326588219/figure/fig1/AS:652185784295425@1532504616288/Continuous-Bag-of-words-CBOW-CB-and-Skip-gram-SG-training-model-illustrations.png\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we can do with it?\n",
    "\n",
    "To calculate the proximity of words, usually the cosine or euclidean distances between vectors are used. Using word embeddings, we can build semantic proportions (also known as analogies) and solve examples like:\n",
    "\n",
    "$$\n",
    "\\textit{king: male = queen: female}  \\\\\n",
    "\\Downarrow \\\\\n",
    "\\textit{king - man + woman = queen}\n",
    "$$\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/d136b7862ae0c1c6e55c2218bfd6749b5b927898a8bd696158bad6af6f58794f/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f323630302f312a73584e5859664171664c556569445850436f313330772e706e67\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Word2vec CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super().__init__()\n",
    "        self.word_emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.linear = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, context_size) -> (batch_size, context_size, emb_dim)\n",
    "        x = self.word_emb(x)\n",
    "        \n",
    "        # (batch_size, context_size, emb_dim) -> (batch_size, emb_dim)\n",
    "        x = x.sum(dim=1)\n",
    "\n",
    "        # (batch_size, emb_dim) -> (batch_size, vocab_size)\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        return  torch.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Instantiate the model and write a proper training loop. Here are some functions to help you make the data ready for use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ContextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenized_texts, context_size=2):\n",
    "        super().__init__()\n",
    "        # shifted by 2 due to special tokens for padding and unknown tokens\n",
    "        self.word_to_ix = {}\n",
    "        self.word_to_ix['<pad>'] = 0\n",
    "        self.word_to_ix['<unk>'] = 1\n",
    "        for text in tokenized_texts:\n",
    "            self.add_to_vocab(text)\n",
    "        self.context_size = context_size\n",
    "        self.contexts = []\n",
    "        self.targets = []\n",
    "        for text in tokenized_texts:\n",
    "            self.add_to_context_and_target(text)\n",
    "    \n",
    "    def add_to_vocab(self, text):\n",
    "        for word in text:\n",
    "            if word not in self.word_to_ix.keys():\n",
    "                self.word_to_ix[word] = len(self.word_to_ix)\n",
    "    \n",
    "    def add_to_context_and_target(self, text):\n",
    "        # k words to the left and k to the right\n",
    "        k = self.context_size\n",
    "        for i in range(len(text)):\n",
    "            context = [text[i+j] if 0 <= i+j < len(text) else '<pad>' for j in range(-k, k+1) if j != 0]\n",
    "            target = text[i]\n",
    "            self.contexts.append(self.get_words_ids(context))\n",
    "            self.targets.append(self.get_word_id(target))\n",
    "    \n",
    "    def get_word_id(self, word):\n",
    "        if word in self.word_to_ix.keys():\n",
    "            return self.word_to_ix[word]\n",
    "        return self.word_to_ix['<unk>']\n",
    "\n",
    "    def get_words_ids(self, words):\n",
    "        return [self.get_word_id(w) for w in words]\n",
    "    \n",
    "    @property\n",
    "    def ix_to_word(self):\n",
    "        return list(self.word_to_ix.keys())\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.word_to_ix)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        context = torch.tensor(self.contexts[idx], dtype=torch.long)\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "        return context, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = [\n",
    "    \"we are about to study the idea of a computational process .\",\n",
    "    \"computational processes are abstract beings that inhabit computers .\",\n",
    "    \"as they evolve, processes manipulate other abstract things called data .\",\n",
    "    \"the evolution of a process is directed by a pattern of rules called a program .\",\n",
    "    \"people create programs to direct processes .\", \n",
    "    \"in effect , we conjure the spirits of the computer with our spells .\"\n",
    "]\n",
    "tokenized_texts = [text.lower().split() for text in raw_texts]\n",
    "\n",
    "train_dataset = ContextDataset(tokenized_texts, context_size=2)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "vocab = train_dataset.word_to_ix\n",
    "print('Dataset size:', len(train_dataset))\n",
    "print('Vocab size:', train_dataset.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 2\n",
    "lr = 0.1\n",
    "\n",
    "model = CBOW(train_dataset.vocab_size, emb_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, loss_function, num_epochs=1):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print('Starting epoch %d' % epoch)\n",
    "        total_loss = 0\n",
    "        hits = 0\n",
    "        for batch_x, batch_y in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_x)\n",
    "            loss = loss_function(logits, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_value = loss.item()\n",
    "            total_loss += loss_value\n",
    "            losses.append(loss_value)\n",
    "            y_pred = logits.argmax(dim=1)\n",
    "            hits += torch.sum(y_pred == batch_y).item()\n",
    "        avg_loss = total_loss / len(train_dataloader.dataset)\n",
    "        print('Epoch loss: %.4f' % avg_loss)\n",
    "        acc = hits / len(train_dataloader.dataset)\n",
    "        print('Epoch accuracy: %.4f' % acc)\n",
    "    print('Done!')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = train_model(model, train_dataloader, optimizer, loss_function, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(losses, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot vectors\n",
    "\n",
    "Since we mapped words to 2D vectors, we can actually plot them. In the real world, however, we would use much larger vector dimensionalities, so we would need some sort of dimensionality reduction algorithm to see a plot like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(w):\n",
    "    return model.word_emb(torch.tensor(vocab[w]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    for w in train_dataset.word_to_ix:\n",
    "        vec = get_vector(w)\n",
    "        ax.plot(vec[0], vec[1], 'k.')\n",
    "        ax.annotate(w, (vec[0], vec[1]), textcoords=\"offset points\", xytext=(0, 5), ha='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding closest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(word, n=10):\n",
    "    vec = get_vector(word)\n",
    "    all_dists = [(w, torch.dist(vec, get_vector(w)).item()) for w in vocab.keys()]\n",
    "    return sorted(all_dists, key=lambda t: t[1])[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest('program', n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try to implement the SkipGram approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "If you like, these PyTorch's NLP tutorials are a good place to start building NLP models:\n",
    "\n",
    "- https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "- https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "- https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    "- https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "- https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
