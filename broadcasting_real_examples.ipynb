{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5b1ca63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99af0cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb8648",
   "metadata": {},
   "source": [
    "# From Puzzles to Real Code\n",
    "\n",
    "All of these puzzles induces you to think about a smart way of using broadcast rules. But turns out that broadcasting is not only useful to solve \"puzzles\". To illustrate this better, here I'm going to show you two code snippets that I took from my recent research projects. For both problems, I've used broadcasting to write an optimized version of standard PyTorch functions. \n",
    "\n",
    "Since in real problems we usually have tensors with a batch dimension to leverage GPUs, these optimizations have to deal with the `batch` dimension. Note also that contrary to some problems, the ones covered here cannot be solved via reshaping since each sequence in the batch is an independent example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeb3637",
   "metadata": {},
   "source": [
    "# Aggregating word pieces\n",
    "\n",
    "When we tokenize texts into word pieces (e.g., BPEs), we end up splitting not only one word from another, but also pieces of the word itself. For example:\n",
    "\n",
    "```python\n",
    ">>> wordpiece_tokenize(\"Welcome to the jungle\")\n",
    "[\"_Wel\", \"come\", \"_to\", \"_the\", \"_jungle\"]\n",
    "```\n",
    "\n",
    "The symbol `_` represents the first piece of a tokenized word. Word piece tokenization has some advantages such as limiting the size of the vocabulary. However, consider a word labelling problem where each word is associated with a label, such as POS tagging or NER. A direct consequence of using word piece tokenization is that the number of \"tokens\" $m$ becomes larger than the actual number of words/labels $n$. Therefore, we need to **map** the tokenized pieces back to their actual words, such that $m = n$ again.\n",
    "\n",
    "A simple way to solve this problem is following the strategy adopted by BERT: we only select the information from the first word piece. For example:\n",
    "\n",
    "```python\n",
    ">>> map_pieces_to_words([\"_Wel\", \"come\", \"_to\", \"_the\", \"_jungle\"])\n",
    "[\"_Wel\", \"_to\", \"_the\", \"_jungle\"]\n",
    "```\n",
    "\n",
    "And so `len(map_pieces_to_words(pieces)) == len(input.split())`.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Consider a model that receives three input tensors:\n",
    "\n",
    "- `input_ids`, a `torch.IntTensor` with a shape of `(batch_size, sequence_length)`\n",
    "- `attention_mask`, a `torch.IntTensor` with a shape of `(batch_size, sequence_length)`\n",
    "- `first_piece_mask`, a `torch.IntTensor` with a shape of `(batch_size, sequence length)`\n",
    "\n",
    "`input_ids` contains indices of word pieces in the vocabulary. `attention_mask` contains boolean values denoting valid and padded positions. `first_piece_mask` contains boolean values denoting whether a token is the first word piece of that word or not. All tensors are properly padded to the right. For example:\n",
    "\n",
    "```python\n",
    "input_texts = [\"Welcome to the jungle\", \"Hello darkness my old friend\"]\n",
    "input_pieces = [wordpiece_tokenize(text) for text in input_texts]\n",
    "```\n",
    "\n",
    "Let's say that the output of this code would be:\n",
    "```python\n",
    ">>> input_pieces\n",
    "[\n",
    "    [\"_Wel\", \"come\", \"_to\", \"_the\", \"_jungle\"],\n",
    "    [\"_He\", \"llo\", \"_dark\", \"ness\", \"_my\", \"_old\", \"_fri\", \"end\"]\n",
    "]\n",
    "```\n",
    "\n",
    "Creating our inputs:\n",
    "```python\n",
    ">>> input_ids = pad([pieces_to_ids(pieces) for pieces in input_pieces], pad_value=-1)\n",
    ">>> input_ids\n",
    "[\n",
    "    [10, 11, 12, 13, 14, -1, -1, -1], \n",
    "    [15, 16, 17, 18, 19, 20, 21, 22]\n",
    "]\n",
    "\n",
    ">>> attention_mask = pad([[True]*len(pieces) for pieces in input_pieces], pad_value=0)\n",
    ">>> attention_mask\n",
    "[\n",
    "    [1, 1, 1, 1, 1, 0, 0, 0], \n",
    "    [1, 1, 1, 1, 1, 1, 1, 1]\n",
    "]\n",
    "\n",
    ">>> first_piece_mask = pad([[p.startswith('_') for p in pieces] for pieces in input_pieces], pad_value=0)\n",
    ">>> first_piece_mask\n",
    "[\n",
    "    [1, 0, 1, 1, 1, 0, 0, 0], \n",
    "    [1, 0, 1, 0, 1, 1, 1, 0]\n",
    "]\n",
    "```\n",
    "\n",
    "Creating tensors:\n",
    "\n",
    "```python\n",
    ">>> input_ids = torch.as_tensor(input_ids)\n",
    ">>> attention_mask = torch.as_tensor(attention_mask)\n",
    ">>> first_piece_mask = torch.as_tensor(first_piece_mask)\n",
    ">>> input_ids.shape  # batch_size = 2, sequence_length = 8\n",
    "torch.Size([2, 8])\n",
    "```\n",
    "\n",
    "That is it. Our setup is done. In actual code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8967c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\"Welcome to the jungle\", \"Hello darkness my old friend\"]\n",
    "input_pieces = [[\"_Wel\", \"come\", \"_to\", \"_the\", \"_jungle\"], \n",
    "                [\"_He\", \"llo\", \"_dark\", \"ness\", \"_my\", \"_old\", \"_fri\", \"end\"]]\n",
    "input_ids = torch.as_tensor([[10, 11, 12, 13, 14, -1, -1, -1], [15, 16, 17, 18, 19, 20, 21, 22]])\n",
    "attention_mask = torch.as_tensor([[1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "first_piece_mask = torch.as_tensor([[1, 0, 1, 1, 1, 0, 0, 0], [1, 0, 1, 0, 1, 1, 1, 0]])\n",
    "batch_size, seq_len = input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc39b91",
   "metadata": {},
   "source": [
    "### First word-piece selection\n",
    "\n",
    "Now, how can we efficiently select the input ids of the **first word piece** for each sequence in the batch?\n",
    "\n",
    "Note that a simple binary-indexing strategy might gives us trouble for two reasons:\n",
    "1. The number of 1s for each sequence in the batch differ.\n",
    "2. Even if the number of 1s was equal for all sequences in the batch, binary indexing does not return a tensor with the same shape as the original tensor.\n",
    "\n",
    "If you try that out, the result would be the following:\n",
    "```python\n",
    ">>> input_ids[first_piece_mask.bool()]\n",
    "tensor([10, 12, 13, 14, 15, 17, 19, 20, 21])\n",
    "```\n",
    "\n",
    "Which is not what we want. To circumvent this behavior, we can resort to positional indexing. That is, we want to select the elements in the following positions:\n",
    "```python\n",
    "[\n",
    "    [0, 2, 3, 4], \n",
    "    [0, 2, 4, 5, 6]\n",
    "]\n",
    "```\n",
    "\n",
    "But here we also face the first issue, namely, that the number of \"selected ids\" is different for the two sequences in the batch. A simple fix to this problem is to _pad_ the first tensor with a dummy index value (e.g., `-1`).\n",
    "\n",
    "Therefore, we are looking for a function that is a vectorized version of `torch.nonzero`. In simple words, we want a function that returns the indices of nonzero input elements as a padded tensor. Here are two ways of achieving this ,one using for loops + `torch.nonzero`, and another using broadcasting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be39ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad(sequences, pad_value=0):\n",
    "    return pad_sequence(sequences, batch_first=True, padding_value=pad_value)\n",
    "\n",
    "def simple_padded_nonzero(mask: torch.LongTensor, pad_value: int = -1) -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Returns a (right-padded) tensor containing indices of nonzero elements from a binary mask tensor.\n",
    "    \n",
    "    Example:\n",
    "        [[1, 0, 0, 1, 0, 1, 0, 1],\n",
    "         [1, 1, 0, 0, 0, 1, 0, 0]]\n",
    "        will be transformed to:\n",
    "        [[0, 3, 5, 7],\n",
    "         [0, 1, 5, -1]]\n",
    "        where -1 indicates pad positions.\n",
    "    \n",
    "    Args:\n",
    "        mask: torch.LongTensor with shape of (batch_size, sequence_length)\n",
    "    \n",
    "    Returns:\n",
    "        torch.LongTensor with shape of (batch_size, original_sequence_length)\n",
    "        where original_sequence_length = max(sum(mask, dim=-1))\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = mask.shape\n",
    "    non_zero_tensors = [torch.nonzero(mask[i]).flatten() for i in range(batch_size)]\n",
    "    return pad(non_zero_tensors, pad_value).to(mask.device)\n",
    "\n",
    "def vectorized_padded_nonzero(mask: torch.LongTensor, pad_value: int = -1) -> torch.LongTensor:\n",
    "    non_zeros = mask.nonzero()\n",
    "    non_zero_rows = non_zeros[:, 0]\n",
    "    non_zero_cols = non_zeros[:, 1]\n",
    "    count_unique_ids = non_zero_rows.bincount().cumsum(dim=0).cpu()\n",
    "    non_zero_tensors = non_zero_cols.tensor_split(count_unique_ids[:-1])\n",
    "    return pad(non_zero_tensors, pad_value).to(mask.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78820f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_piece_idxs = simple_padded_nonzero(first_piece_mask)\n",
    "first_piece_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_piece_idxs = vectorized_padded_nonzero(first_piece_mask)\n",
    "first_piece_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfebf6a7",
   "metadata": {},
   "source": [
    "Great! Both versions return the same output. Later on we will compare everything in terms of running time to see the impact of vectorizing. Now that we have the indices of the first pieces, we can simply do an index selection as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8caa514",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = torch.arange(batch_size)\n",
    "input_ids[ar.unsqueeze(-1), first_piece_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cc1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask[ar.unsqueeze(-1), first_piece_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb6ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, 2, size=(128, 512))\n",
    "%timeit simple_padded_nonzero(x.cpu())\n",
    "%timeit vectorized_padded_nonzero(x.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a4c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit simple_padded_nonzero(x.cuda())\n",
    "%timeit vectorized_padded_nonzero(x.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209029d",
   "metadata": {},
   "source": [
    "### Summing and averaging pieces\n",
    "\n",
    "Instead of selecting only the first piece of each word, we can think of other aggregation strategies, such as summing or averaging all piece vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_piece_mask)\n",
    "print(first_piece_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf36198",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_piece_mask_aug = first_piece_mask + (1 - attention_mask)\n",
    "last_piece_mask = ((first_piece_mask_aug  - first_piece_mask_aug.roll(-1)) <= 0).long() * attention_mask\n",
    "last_piece_idxs = vectorized_padded_nonzero(last_piece_mask)\n",
    "print(last_piece_mask)\n",
    "print(last_piece_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c933ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsummed_pieces = input_ids.cumsum(dim=1)\n",
    "cumsummed_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b20add",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cumsummed_pieces[ar.unsqueeze(-1), last_piece_idxs]\n",
    "shifted_cumsummed_pieces = torch.cat((torch.zeros(batch_size, 1), cumsummed_pieces[:, :-1]), dim=1)\n",
    "summed_pieces = a - shifted_cumsummed_pieces[ar.unsqueeze(-1), first_piece_idxs]\n",
    "summed_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a7c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = last_piece_idxs - first_piece_idxs + 1\n",
    "summed_pieces / lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4225cac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Clustered attention\n",
    "\n",
    "Self-attention in transformers work with 3 tensors:\n",
    "\n",
    "- `queries` with a shape of `(batch_size, num_heads, sequence_length, hidden_size)`\n",
    "- `keys` with a shape of `(batch_size, num_heads, sequence_length, hidden_size)`\n",
    "- `values` with a shape of `(batch_size, num_heads, sequence_length, hidden_size)`\n",
    "\n",
    "Attention is computed as follows:\n",
    "\n",
    "```python\n",
    "# 1. compute logits in O(n^2 * d)\n",
    "logits = queries @ keys.transpose(-1, -2) / math.sqrt(hidden_size)\n",
    "# 2. mask out padding positions\n",
    "logits = torch.masked_fill(attention_mask == 0, -9999999.)\n",
    "# 3. map logits to probabilities\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "# 4. compute a weighted sum of value vectors\n",
    "output = probas @ values\n",
    "```\n",
    "\n",
    "Let's say we want to improve the self-attention performance in transformers by working with clusters. The idea is that instead of compution all $n \\times n$ dot-products, we can map queries, keys, and values to some clusters, and then compute dot-product only inside those clusters. Concretely, if we have **balanced** $c$ clusters, we can reduce the self-attention cost to:\n",
    "\n",
    "$$\n",
    "O \\left(c \\times \\frac{n}{c} \\times \\frac{n}{c} \\times d \\right) = O\\left(\\frac{n^2}{c} \\times d\\right)\n",
    "$$\n",
    "\n",
    "If we set $c = \\sqrt{n}$, which is a reasonable choice for the number of clusters for most applications, we get $O(n\\sqrt{n} \\times d)$, which is better than the quadractic cost $O(n^2 \\times d)$.\n",
    "\n",
    "\n",
    "In pratical terms, consider that you are given $c$ clusters as represented by their centroids for each head:\n",
    "\n",
    "- `centroids` with a shape of `(num_heads, num_centroids, hidden_size)`\n",
    "\n",
    "How can we compute attention efficiently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0e803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustered_attention_sorted(q, k, v, centroids, mask):\n",
    "    # get sequence lengths for q and k (might be different for seq2seq problems)\n",
    "    q_seq_len = q.shape[-2]\n",
    "    k_seq_len = k.shape[-2]\n",
    "    batch_size = q.shape[0]\n",
    "    num_heads = q.shape[1]\n",
    "    num_centroids = centroids.shape[-2]\n",
    "    \n",
    "    # add `batch` dimension\n",
    "    # (batch_size, num_heads, 1, num_centroids, num_projections)\n",
    "    expanded_centroids = centroids[None, :, None, :, :].expand(batch_size, -1, 1, -1, -1)\n",
    "\n",
    "    # add `middle` dimension\n",
    "    # (batch_size, num_heads, 1, q_seq_len, num_projections)\n",
    "    expanded_q = q.unsqueeze(2)\n",
    "    # (batch_size, num_heads, 1, k_seq_len, num_projections)\n",
    "    expanded_k = k.unsqueeze(2)\n",
    "\n",
    "    # q_dists.shape is (batch, num_heads, 1, q_seq_len, num_centroids)\n",
    "    q_dists = torch.cdist(expanded_q, expanded_centroids, p=2)\n",
    "    # k_dists.shape is (batch, num_heads, 1, k_seq_len, num_centroids)\n",
    "    k_dists = torch.cdist(expanded_k, expanded_centroids, p=2)\n",
    "\n",
    "    # q_clustered.shape is (batch, num_heads, 1, q_seq_len)\n",
    "    q_clustered = torch.argmin(q_dists, dim=-1)\n",
    "    # k_clustered.shape is (batch, num_heads, 1, k_seq_len)\n",
    "    k_clustered = torch.argmin(k_dists, dim=-1)\n",
    "\n",
    "    # transpose to get `1` as different hashing rounds\n",
    "    # q_clustered.shape is (batch, num_heads, q_seq_len, 1)\n",
    "    q_clustered = q_clustered.transpose(2, 3)\n",
    "    # k_clustered.shape is (batch, num_heads, k_seq_len, 1)\n",
    "    k_clustered = k_clustered.transpose(2, 3)\n",
    "    \n",
    "    # deal with mask later, but we can also\n",
    "    # set cluster id for padding positions as `num_centroids` (ids start with 0)\n",
    "    # q_clustered = q_clustered.masked_fill(~mask.view(batch_size, 1, q_seq_len, 1), num_centroids)\n",
    "    # k_clustered = k_clustered.masked_fill(~mask.view(batch_size, 1, k_seq_len, 1), num_centroids)\n",
    "\n",
    "    # we need to divide q_clustered into (similarly for k_clustered)\n",
    "    # (batch, num_heads, num_centroids, max_cluster_size_q_for_all_batch_and_heads, 1)\n",
    "\n",
    "    # q_clustered_bin.shape is (batch, num_heads, q_seq_len, num_centroids)\n",
    "    q_clustered_bin = q_clustered == torch.arange(num_centroids, device=device)\n",
    "    # k_clustered_bin.shape is (batch, num_heads, k_seq_len, num_centroids)\n",
    "    k_clustered_bin = k_clustered == torch.arange(num_centroids, device=device)\n",
    "\n",
    "    # q_clustered_bin.shape is (batch, num_heads, num_centroids, q_seq_len)\n",
    "    q_clustered_bin = q_clustered_bin.transpose(-1, -2).int()\n",
    "    # k_clustered_bin.shape is (batch, num_heads, num_centroids, k_seq_len)\n",
    "    k_clustered_bin = k_clustered_bin.transpose(-1, -2).int()\n",
    "\n",
    "    # get the max cluster size across all batches and heads\n",
    "    max_cluster_size_q = q_clustered_bin.sum(-1).max().item()\n",
    "    max_cluster_size_k = k_clustered_bin.sum(-1).max().item()\n",
    "\n",
    "    # utopically, max_cluster_size_q = q_seq_len / num_centroids\n",
    "    # but in this implementation I'm ignoring this assumption\n",
    "    # `q_clustered_vals` contains only 0 or 1 ints (due to one hot binarization)\n",
    "    q_clustered_vals, q_clustered_idxs = q_clustered_bin.sort(dim=-1, descending=True, stable=True)\n",
    "    k_clustered_vals, k_clustered_idxs = k_clustered_bin.sort(dim=-1, descending=True, stable=True)\n",
    "    # values that are 0 correspond to padding positions, so we mask them with q_seq_len - 1 (last token)\n",
    "    q_clustered_idxs[~q_clustered_vals.bool()] = q_seq_len - 1\n",
    "    k_clustered_idxs[~k_clustered_vals.bool()] = k_seq_len - 1\n",
    "    # get 0 and 1s as masks\n",
    "    mask_clustered_q = q_clustered_vals.bool()\n",
    "    mask_clustered_k = k_clustered_vals.bool()\n",
    "\n",
    "    # deal with padding\n",
    "    lenghts = mask.sum(-1)[:, None, None, None]\n",
    "    pad_mask_bucketed_q = q_clustered_idxs < lenghts\n",
    "    pad_mask_bucketed_k = k_clustered_idxs < lenghts\n",
    "    \n",
    "    # combine masks\n",
    "    full_mask_bucketed_q = mask_clustered_q & pad_mask_bucketed_q\n",
    "    full_mask_bucketed_k = mask_clustered_k & pad_mask_bucketed_k\n",
    "\n",
    "    # q_bucketed.shape is (batch, num_heads, num_centroids, max_cluster_size_q)\n",
    "    q_bucketed = q_clustered_idxs[:, :, :, :max_cluster_size_q]\n",
    "    # k_bucketed.shape is (batch, num_heads, num_centroids, max_cluster_size_k)\n",
    "    k_bucketed = k_clustered_idxs[:, :, :, :max_cluster_size_k]\n",
    "    # same shape as above\n",
    "    mask_bucketed_q = mask_clustered_q[:, :, :, :max_cluster_size_q]\n",
    "    mask_bucketed_k = mask_clustered_k[:, :, :, :max_cluster_size_k]\n",
    "    full_mask_bucketed_q = full_mask_bucketed_q[:, :, :, :max_cluster_size_q]\n",
    "    full_mask_bucketed_k = full_mask_bucketed_k[:, :, :, :max_cluster_size_k]\n",
    "    # create pairwise mask with shape (batch, num_heads, num_centroids, max_cluster_size_q, max_cluster_size_k)\n",
    "    mask_bucketed = full_mask_bucketed_q.unsqueeze(-1) & full_mask_bucketed_k.unsqueeze(-2)\n",
    "\n",
    "    # (batch, num_heads, num_clusters * max_cluster_size)\n",
    "    squished_inds_q = q_bucketed.reshape(batch_size, num_heads, -1)\n",
    "    squished_inds_k = k_bucketed.reshape(batch_size, num_heads, -1)\n",
    "\n",
    "    # keys and values are bucketed with the same buckets\n",
    "    # the bucketed tensors are (batch, num_heads, num_clusters * max_cluster_size, head_size)\n",
    "    bucketed_q = q.gather(2, squished_inds_q.unsqueeze(-1).expand(-1, -1, -1, head_size))\n",
    "    bucketed_k = k.gather(2, squished_inds_k.unsqueeze(-1).expand(-1, -1, -1, head_size))\n",
    "    bucketed_v = v.gather(2, squished_inds_k.unsqueeze(-1).expand(-1, -1, -1, head_size))\n",
    "\n",
    "    # we now expand the squished dim into (num_centroids, max_cluster_size)\n",
    "    bucketed_q = bucketed_q.view(batch_size, num_heads, num_centroids, -1, head_size)\n",
    "    bucketed_k = bucketed_k.view(batch_size, num_heads, num_centroids, -1, head_size)\n",
    "    bucketed_v = bucketed_v.view(batch_size, num_heads, num_centroids, -1, head_size)\n",
    "\n",
    "    # dots are (batch, num_heads, num_centroids, max_cluster_size_q, max_cluster_size_k)\n",
    "    sqrt_d = head_size ** 0.5\n",
    "    dots = bucketed_q @ bucketed_k.transpose(-1, -2) / sqrt_d\n",
    "\n",
    "    # mask the dots past key length; add `max_cluster_size_q` dim for broadcasting\n",
    "    neg_inf = -9999999.0\n",
    "    dots = dots.masked_fill(~mask_bucketed, neg_inf)  # float('-inf') will generate nans in softmax\n",
    "\n",
    "    # att_dist is (batch, num_heads, num_centroids, max_cluster_size_q, max_cluster_size_k)\n",
    "    att_dist = torch.softmax(dots, dim=-1)\n",
    "\n",
    "    # fix the uniform numbers for padding positions\n",
    "    att_dist = att_dist * mask_bucketed.float()\n",
    "\n",
    "    # output is (batch, num_heads, num_centroids, max_cluster_size_q, head_size)\n",
    "    output = torch.matmul(att_dist, bucketed_v)\n",
    "\n",
    "    # make sure squashed indices for pad positions are higher than last valid token id\n",
    "    squished_mask_q = mask_bucketed_q.reshape(batch_size, num_heads, -1)\n",
    "    # squished_mask_k = mask_bucketed_k.reshape(batch_size, num_heads, -1)\n",
    "    fixed_squished_inds_q = squished_inds_q.masked_fill(~squished_mask_q, q_seq_len + 1)\n",
    "    # fixed_squished_inds_k = squished_inds_q.masked_fill(~squished_mask_k, k_seq_len + 1)\n",
    "\n",
    "    # get indices of valid contextualized query vectors\n",
    "    _, rev_inds_q = fixed_squished_inds_q.sort(dim=-1, stable=True)\n",
    "    # truncate to get only the first q_seq_len vectors -> the valid ones\n",
    "    rev_inds_q = rev_inds_q[:, :, :q_seq_len]\n",
    "    # fix order\n",
    "    rev_inds_q, _ = rev_inds_q.sort(dim=-1)\n",
    "\n",
    "    # squish output and gather correct vectors\n",
    "    squished_output = output.view(batch_size, num_heads, -1, head_size)\n",
    "    # output.shape is (batch, num_heads, q_seq_len, head_size)\n",
    "    output = squished_output.gather(2, rev_inds_q.unsqueeze(-1).expand(-1, -1, -1, head_size))\n",
    "\n",
    "    # concat heads back\n",
    "    output = output.transpose(1, 2).reshape(batch_size, -1, num_heads * head_size)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab9a5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad(sequences, pad_value=0):\n",
    "    return pad_sequence(sequences, batch_first=True, padding_value=pad_value)\n",
    "\n",
    "def clustered_attention_vectorized(q, k, v, centroids, mask):\n",
    "    # get sequence lengths for q and k (might be different for seq2seq problems)\n",
    "    q_seq_len = q.shape[-2]\n",
    "    k_seq_len = k.shape[-2]\n",
    "    batch_size = q.shape[0]\n",
    "    num_heads = q.shape[1]\n",
    "    num_centroids = centroids.shape[-2]\n",
    "    \n",
    "    # add `batch` dimension\n",
    "    # (batch_size, num_heads, 1, num_centroids, num_projections)\n",
    "    expanded_centroids = centroids[None, :, None, :, :].expand(batch_size, -1, 1, -1, -1)\n",
    "\n",
    "    # add `middle` dimension\n",
    "    # (batch_size, num_heads, 1, q_seq_len, num_projections)\n",
    "    expanded_q = q.unsqueeze(2)\n",
    "    # (batch_size, num_heads, 1, k_seq_len, num_projections)\n",
    "    expanded_k = k.unsqueeze(2)\n",
    "\n",
    "    # q_dists.shape is (batch, num_heads, 1, q_seq_len, num_centroids)\n",
    "    q_dists = torch.cdist(expanded_q, expanded_centroids, p=2)\n",
    "    # k_dists.shape is (batch, num_heads, 1, k_seq_len, num_centroids)\n",
    "    k_dists = torch.cdist(expanded_k, expanded_centroids, p=2)\n",
    "\n",
    "    # q_clustered.shape is (batch, num_heads, 1, q_seq_len)\n",
    "    q_clustered = torch.argmin(q_dists, dim=-1)\n",
    "    # k_clustered.shape is (batch, num_heads, 1, k_seq_len)\n",
    "    k_clustered = torch.argmin(k_dists, dim=-1)\n",
    "\n",
    "    # transpose to get `1` as different hashing rounds\n",
    "    # q_clustered.shape is (batch, num_heads, q_seq_len, 1)\n",
    "    q_clustered = q_clustered.transpose(2, 3)\n",
    "    # k_clustered.shape is (batch, num_heads, k_seq_len, 1)\n",
    "    k_clustered = k_clustered.transpose(2, 3)\n",
    "    \n",
    "    # we will deal with masking later, but we could\n",
    "    # set cluster id for padding positions as `num_centroids` (ids start with 0)\n",
    "    # q_clustered = q_clustered.masked_fill(~mask.view(batch_size, 1, q_seq_len, 1), num_centroids)\n",
    "    # k_clustered = k_clustered.masked_fill(~mask.view(batch_size, 1, k_seq_len, 1), num_centroids)\n",
    "\n",
    "    # we need to divide q_clustered into (similarly for k_clustered)\n",
    "    # (batch, num_heads, num_centroids, max_cluster_size_q_for_all_batch_and_heads, 1)\n",
    "\n",
    "    # q_clustered_bin.shape is (batch, num_heads, q_seq_len, num_centroids)\n",
    "    q_clustered_bin = q_clustered == torch.arange(num_centroids, device=device)\n",
    "    # k_clustered_bin.shape is (batch, num_heads, k_seq_len, num_centroids)\n",
    "    k_clustered_bin = k_clustered == torch.arange(num_centroids, device=device)\n",
    "\n",
    "    # q_clustered_bin.shape is (batch, num_heads, num_centroids, q_seq_len)\n",
    "    q_clustered_bin = q_clustered_bin.transpose(-1, -2).int()\n",
    "    # k_clustered_bin.shape is (batch, num_heads, num_centroids, k_seq_len)\n",
    "    k_clustered_bin = k_clustered_bin.transpose(-1, -2).int()\n",
    "    \n",
    "    # arange tensors for queries and keys\n",
    "    q_ar = 1 + torch.arange(q_seq_len, device=device).view(1, 1, 1, -1).expand_as(q_clustered_bin)\n",
    "    k_ar = 1 + torch.arange(k_seq_len, device=device).view(1, 1, 1, -1).expand_as(k_clustered_bin)\n",
    "    \n",
    "    # q_nz.shape is (num_now_zero_entries, 4)\n",
    "    # where each column contains the nonzero ids for each original dimension,\n",
    "    # namely: batch, num_heads, num_centroids, q_seq_len\n",
    "    q_nz = (q_ar * q_clustered_bin).nonzero()\n",
    "    k_nz = (k_ar * k_clustered_bin).nonzero()\n",
    "    \n",
    "    # convert the first three columns into a single column\n",
    "    q_rows = q_nz[:, 0] * (num_heads * num_centroids) + q_nz[:, 1] * num_centroids + q_nz[:, 2]\n",
    "    k_rows = k_nz[:, 0] * (num_heads * num_centroids) + k_nz[:, 1] * num_centroids + k_nz[:, 2]\n",
    "    \n",
    "    # the last column is the sequence dimension (the one we care about) \n",
    "    q_cols = q_nz[:, -1]\n",
    "    k_cols = k_nz[:, -1]\n",
    "    \n",
    "    # count the number of unique row ids and cumsum them to create continuous slices \n",
    "    q_split_slices = q_rows.bincount().cumsum(dim=0)[:-1].cpu().tolist()\n",
    "    k_split_slices = k_rows.bincount().cumsum(dim=0)[:-1].cpu().tolist()\n",
    "    \n",
    "    # pad for missing slices since the last head of the last batch might be empty\n",
    "    num_total_centroids = batch_size * num_heads * num_centroids\n",
    "    q_num_missing_centroids = num_total_centroids - len(q_split_slices)\n",
    "    k_num_missing_centroids = num_total_centroids - len(k_split_slices)\n",
    "    q_split_slices.extend([q_split_slices[-1]] * (q_num_missing_centroids - 1))\n",
    "    k_split_slices.extend([k_split_slices[-1]] * (k_num_missing_centroids - 1))\n",
    "    \n",
    "    # merge the sequence ids in tensors following the slices\n",
    "    q_splited = q_cols.tensor_split(q_split_slices)\n",
    "    k_splited = k_cols.tensor_split(k_split_slices)\n",
    "    \n",
    "    # pad the smaller tensors with -1 and reshape back to \n",
    "    # (batch_size, num_heads, num_centroids, max_cluster_size_q_for_all_batch_and_heads)\n",
    "    q_bucketed_idxs = pad(q_splited, -1).view(batch_size, num_heads, num_centroids, -1)\n",
    "    k_bucketed_idxs = pad(k_splited, -1).view(batch_size, num_heads, num_centroids, -1)\n",
    "    \n",
    "    # get the max cluster size across all batches and heads\n",
    "    # utopically, max_cluster_size_q = q_seq_len / num_centroids\n",
    "    # but in this implementation I'm ignoring this assumption\n",
    "    max_cluster_size_q = q_bucketed_idxs.shape[-1]\n",
    "    max_cluster_size_k = k_bucketed_idxs.shape[-1]\n",
    "    mask_bucketed_q = q_bucketed_idxs != -1\n",
    "    mask_bucketed_k = k_bucketed_idxs != -1\n",
    "    \n",
    "    # deal with padding\n",
    "    lenghts = mask.sum(-1)[:, None, None, None]\n",
    "    pad_mask_bucketed_q = q_bucketed_idxs < lenghts\n",
    "    pad_mask_bucketed_k = k_bucketed_idxs < lenghts\n",
    "\n",
    "    # combine masks\n",
    "    full_mask_bucketed_q = mask_bucketed_q & pad_mask_bucketed_q\n",
    "    full_mask_bucketed_k = mask_bucketed_k & pad_mask_bucketed_k\n",
    "    \n",
    "    # create pairwise mask with shape \n",
    "    # (batch, num_heads, num_centroids, max_cluster_size_q, max_cluster_size_k)\n",
    "    # this is where having balanced clusters with num_centroids = sqrt(n)\n",
    "    # leads to performance improvements\n",
    "    mask_bucketed = full_mask_bucketed_q.unsqueeze(-1) & full_mask_bucketed_k.unsqueeze(-2)\n",
    "\n",
    "    # (batch, num_heads, num_clusters * max_cluster_size)\n",
    "    q_bucketed = q_bucketed_idxs.masked_fill(~mask_bucketed_q, q_seq_len - 1)\n",
    "    k_bucketed = k_bucketed_idxs.masked_fill(~mask_bucketed_k, k_seq_len - 1)\n",
    "    squished_inds_q = q_bucketed.reshape(batch_size, num_heads, -1)\n",
    "    squished_inds_k = k_bucketed.reshape(batch_size, num_heads, -1)\n",
    "\n",
    "    # keys and values are bucketed with the same ids\n",
    "    # the bucketed tensors are (batch, num_heads, num_clusters * max_cluster_size, head_size)\n",
    "    bucketed_q = q.gather(2, squished_inds_q.unsqueeze(-1).expand(-1, -1, -1, head_size))\n",
    "    bucketed_k = k.gather(2, squished_inds_k.unsqueeze(-1).expand(-1, -1, -1, head_size))\n",
    "    bucketed_v = v.gather(2, squished_inds_k.unsqueeze(-1).expand(-1, -1, -1, head_size))\n",
    "\n",
    "    # we now expand the squished dim into (num_centroids, max_cluster_size)\n",
    "    bucketed_q = bucketed_q.view(batch_size, num_heads, num_centroids, -1, head_size)\n",
    "    bucketed_k = bucketed_k.view(batch_size, num_heads, num_centroids, -1, head_size)\n",
    "    bucketed_v = bucketed_v.view(batch_size, num_heads, num_centroids, -1, head_size)\n",
    "\n",
    "    # dots are (batch, num_heads, num_centroids, max_cluster_size_q, max_cluster_size_k)\n",
    "    sqrt_d = head_size ** 0.5\n",
    "    dots = bucketed_q @ bucketed_k.transpose(-1, -2) / sqrt_d\n",
    "    # mask the dots past key length; add `max_cluster_size_q` dim for broadcasting\n",
    "    # float('-inf') will generate nans in softmax, so we use a very small value\n",
    "    # instead. This happens because some clusters might be empty\n",
    "    neg_inf = -9999999.0\n",
    "    dots = dots.masked_fill(~mask_bucketed, neg_inf)\n",
    "\n",
    "    # att_dist is (batch, num_heads, num_centroids, max_cluster_size_q, max_cluster_size_k)\n",
    "    att_dist = torch.softmax(dots, dim=-1)\n",
    "\n",
    "    # fix the uniform numbers for padding positions\n",
    "    att_dist = att_dist * mask_bucketed.float()\n",
    "\n",
    "    # output is (batch, num_heads, num_centroids, max_cluster_size_q, head_size)\n",
    "    att_output = torch.matmul(att_dist, bucketed_v)\n",
    "\n",
    "    # squish output and mask\n",
    "    squished_output = att_output.view(batch_size, num_heads, -1, head_size)\n",
    "    squished_mask_q = mask_bucketed_q.view(batch_size, num_heads, -1)\n",
    "    \n",
    "    # get indices of valid contextualized query vectors\n",
    "    ar = torch.arange(num_centroids * max_cluster_size_q, device=device)\n",
    "    ar = ar.view(1, 1, -1).expand(batch_size, num_heads, -1)\n",
    "    squished_idxs_q = ar[squished_mask_q].view(batch_size, num_heads, -1)\n",
    "    \n",
    "    # output.shape is (batch, num_heads, q_seq_len, head_size)\n",
    "    output = squished_output.gather(2, squished_idxs_q.unsqueeze(-1).expand(-1, -1, -1, head_size))\n",
    "    \n",
    "    # concat heads back\n",
    "    # output.shape is (batch, q_seq_len, hidden_size)\n",
    "    output = output.transpose(1, 2).reshape(batch_size, -1, num_heads * head_size)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349ff753",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "sequence_length = 2048\n",
    "num_heads = 4\n",
    "head_size = 4\n",
    "hidden_size = num_heads * head_size\n",
    "num_centroids = 3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "centroids = torch.randn(num_heads, num_centroids, hidden_size).to(device)\n",
    "q = torch.randn(batch_size, num_heads, sequence_length, hidden_size).to(device)\n",
    "k = torch.randn(batch_size, num_heads, sequence_length, hidden_size).to(device)\n",
    "v = torch.randn(batch_size, num_heads, sequence_length, hidden_size).to(device)\n",
    "mask = torch.ones(batch_size, sequence_length).bool().to(device)\n",
    "# mask = torch.tensor([5, 8]).unsqueeze(-1) >= torch.arange(sequence_length).unsqueeze(0).expand(batch_size, -1)\n",
    "# mask = mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f175a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    %timeit clustered_attention_sorted(q, k, v, centroids, mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c2e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    %timeit clustered_attention_vectorized(q, k, v, centroids, mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d3c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del q, k, v, mask, centroids\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741e7bce",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28dae7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------- clustered attention ----------]\n",
      "                   |  sorted  |  vectorized\n",
      "1 threads: --------------------------------\n",
      "      [2, 64]      |    1.0   |      1.4   \n",
      "      [2, 128]     |    1.1   |      1.4   \n",
      "      [2, 512]     |    1.1   |      1.6   \n",
      "      [2, 1024]    |    1.1   |      1.7   \n",
      "      [2, 2048]    |    1.1   |      1.8   \n",
      "      [2, 4096]    |    2.6   |      2.8   \n",
      "      [2, 8192]    |   10.6   |     10.3   \n",
      "      [2, 16384]   |   33.2   |     31.5   \n",
      "      [3, 64]      |    1.0   |      1.4   \n",
      "      [3, 128]     |    1.1   |      1.4   \n",
      "      [3, 512]     |    1.1   |      1.5   \n",
      "      [3, 1024]    |    1.2   |      1.8   \n",
      "      [3, 2048]    |    1.8   |      2.3   \n",
      "      [3, 4096]    |    2.3   |      2.5   \n",
      "      [3, 8192]    |    8.0   |      7.4   \n",
      "      [3, 16384]   |   63.3   |     55.8   \n",
      "      [5, 64]      |    1.0   |      1.4   \n",
      "      [5, 128]     |    1.1   |      1.4   \n",
      "      [5, 512]     |    1.2   |      1.5   \n",
      "      [5, 1024]    |    1.2   |      1.5   \n",
      "      [5, 2048]    |    2.0   |      2.4   \n",
      "      [5, 4096]    |    2.3   |      2.4   \n",
      "      [5, 8192]    |    5.5   |      4.8   \n",
      "      [5, 16384]   |   13.4   |     10.8   \n",
      "      [9, 64]      |    1.0   |      1.5   \n",
      "      [9, 128]     |    1.1   |      1.5   \n",
      "      [9, 512]     |    1.2   |      1.5   \n",
      "      [9, 1024]    |    1.2   |      1.6   \n",
      "      [9, 2048]    |    1.9   |      2.2   \n",
      "      [9, 4096]    |    3.4   |      3.0   \n",
      "      [9, 8192]    |    8.8   |      6.3   \n",
      "      [9, 16384]   |   29.1   |     18.8   \n",
      "      [11, 64]     |    1.0   |      1.5   \n",
      "      [11, 128]    |    1.1   |      1.5   \n",
      "      [11, 512]    |    1.2   |      1.5   \n",
      "      [11, 1024]   |    1.2   |      1.5   \n",
      "      [11, 2048]   |    1.4   |      1.8   \n",
      "      [11, 4096]   |    2.7   |      2.5   \n",
      "      [11, 8192]   |   11.2   |      7.2   \n",
      "      [11, 16384]  |   44.3   |     25.7   \n",
      "\n",
      "Times are in milliseconds (ms).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "from itertools import product\n",
    "\n",
    "batch_size = 1\n",
    "num_heads = 1\n",
    "head_size = 1\n",
    "hidden_size = num_heads * head_size\n",
    "sequence_lengths = [64, 128, 512, 1024, 2048, 4096, 8192, 8192*2]\n",
    "num_centroids = [2, 3, 5, 9, 11]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_threads = 1\n",
    "results = []\n",
    "\n",
    "for num_c, seq_len in product(num_centroids, sequence_lengths):\n",
    "    label = 'clustered attention'\n",
    "    sub_label = f'[{num_c}, {seq_len}]'\n",
    "    q = torch.randn(batch_size, num_heads, seq_len, hidden_size).to(device)\n",
    "    k = torch.randn(batch_size, num_heads, seq_len, hidden_size).to(device)\n",
    "    v = torch.randn(batch_size, num_heads, seq_len, hidden_size).to(device)\n",
    "    centroids = torch.randn(num_heads, num_c, hidden_size).to(device)\n",
    "    mask = torch.ones(batch_size, seq_len).bool().to(device)\n",
    "    results.append(benchmark.Timer(\n",
    "        stmt='clustered_attention_sorted(q, k, v, centroids, mask)',\n",
    "        setup='from __main__ import clustered_attention_sorted',\n",
    "        globals={'q': q, 'k': k, 'v': v, 'centroids': centroids, 'mask': mask},\n",
    "        num_threads=num_threads,\n",
    "        label=label,\n",
    "        sub_label=sub_label,\n",
    "        description='sorted',\n",
    "    ).blocked_autorange(min_run_time=1))\n",
    "    results.append(benchmark.Timer(\n",
    "        stmt='clustered_attention_vectorized(q, k, v, centroids, mask)',\n",
    "        setup='from __main__ import clustered_attention_vectorized',\n",
    "        globals={'q': q, 'k': k, 'v': v, 'centroids': centroids, 'mask': mask},\n",
    "        num_threads=num_threads,\n",
    "        label=label,\n",
    "        sub_label=sub_label,\n",
    "        description='vectorized',\n",
    "    ).blocked_autorange(min_run_time=1))\n",
    "\n",
    "compare = benchmark.Compare(results)\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d459a84",
   "metadata": {},
   "source": [
    "--- \n",
    "## Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce131f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "batch_size = 2\n",
    "sequence_length = 1024\n",
    "num_heads = 4\n",
    "head_size = 4\n",
    "hidden_size = num_heads * head_size\n",
    "num_centroids = 3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "centroids = torch.randn(num_heads, num_centroids, hidden_size).to(device)\n",
    "q = torch.randn(batch_size, num_heads, sequence_length, hidden_size).to(device)\n",
    "k = torch.randn(batch_size, num_heads, sequence_length, hidden_size).to(device)\n",
    "v = torch.randn(batch_size, num_heads, sequence_length, hidden_size).to(device)\n",
    "mask = torch.ones(batch_size, sequence_length).bool().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "142bb880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             aten::sort         2.32%     655.000us         4.78%       1.349ms     337.250us       1.826ms        19.32%       2.006ms     501.500us           0 b           0 b     957.00 Kb    -776.00 Kb             4            --  \n",
      "                                              aten::bmm         0.33%      94.000us         0.45%     126.000us      31.500us     850.000us         8.99%     850.000us     212.500us           0 b           0 b      42.15 Mb      42.15 Mb             4       176.729  \n",
      "                                            aten::copy_         1.04%     295.000us         1.80%     509.000us      19.577us     724.000us         7.66%     724.000us      27.846us           0 b           0 b           0 b           0 b            26            --  \n",
      "                                              aten::mul         0.39%     110.000us         0.72%     204.000us      40.800us     563.000us         5.96%     620.000us     124.000us           0 b           0 b      43.71 Mb      42.71 Mb             5        11.459  \n",
      "                                       aten::as_strided         0.93%     263.000us         1.54%     436.000us       5.190us     327.000us         3.46%     327.000us       3.893us           0 b           0 b           0 b           0 b            84            --  \n",
      "                                            aten::slice         0.90%     255.000us         1.86%     525.000us      16.935us     316.000us         3.34%     435.000us      14.032us           0 b           0 b           0 b           0 b            31            --  \n",
      "                                              aten::sum         0.76%     216.000us         1.67%     471.000us      67.286us     316.000us         3.34%     507.000us      72.429us           0 b           0 b      66.50 Kb    -333.50 Kb             7            --  \n",
      "                                  aten::_euclidean_dist         0.86%     244.000us         6.12%       1.727ms     863.500us     276.000us         2.92%       1.725ms     862.500us           0 b           0 b     192.00 Kb      -3.26 Mb             2            --  \n",
      "                                     aten::masked_fill_         0.16%      46.000us         0.26%      74.000us      18.500us     258.000us         2.73%     258.000us      64.500us           0 b           0 b           0 b           0 b             4            --  \n",
      "                                      aten::bitwise_and         0.21%      59.000us         0.29%      82.000us      27.333us     257.000us         2.72%     257.000us      85.667us           0 b           0 b      10.48 Mb      10.48 Mb             3            --  \n",
      "                                         aten::_softmax         0.07%      21.000us         0.11%      30.000us      30.000us     223.000us         2.36%     223.000us     223.000us           0 b           0 b      41.71 Mb      41.71 Mb             1            --  \n",
      "                                            aten::empty         0.95%     267.000us         1.08%     305.000us      14.524us     196.000us         2.07%     196.000us       9.333us           0 b           0 b      42.29 Mb      42.29 Mb            21            --  \n",
      "                                           aten::expand         0.53%     149.000us         1.06%     300.000us      17.647us     189.000us         2.00%     253.000us      14.882us           0 b           0 b           0 b           0 b            17            --  \n",
      "                                              aten::div         0.08%      23.000us         0.11%      30.000us      30.000us     184.000us         1.95%     184.000us     184.000us           0 b           0 b      41.71 Mb      41.71 Mb             1            --  \n",
      "                                    aten::empty_strided         0.95%     269.000us         1.10%     312.000us      13.000us     182.000us         1.93%     182.000us       7.583us           0 b           0 b      44.07 Mb      44.07 Mb            24            --  \n",
      "                                              aten::pow         0.41%     115.000us         0.71%     200.000us      50.000us     164.000us         1.73%     199.000us      49.750us           0 b           0 b       1.00 Mb       1.00 Mb             4            --  \n",
      "                                         aten::_to_copy         0.47%     134.000us         1.80%     509.000us      50.900us     155.000us         1.64%     604.000us      60.400us           0 b           0 b      42.39 Mb           0 b            10            --  \n",
      "                                           aten::gather         0.44%     124.000us         0.73%     205.000us      51.250us     155.000us         1.64%     183.000us      45.750us           0 b           0 b     888.00 Kb     888.00 Kb             4            --  \n",
      "                                           aten::matmul         0.51%     145.000us         2.32%     656.000us     164.000us     153.000us         1.62%       1.242ms     310.500us           0 b           0 b      42.15 Mb           0 b             4            --  \n",
      "                                             aten::_cat         0.28%      80.000us         0.78%     220.000us      55.000us     148.000us         1.57%     234.000us      58.500us           0 b           0 b       1.13 Mb           0 b             4            --  \n",
      "                                            aten::cdist         0.35%      99.000us         7.80%       2.202ms       1.101ms     135.000us         1.43%       2.200ms       1.100ms           0 b           0 b     192.00 Kb      -3.00 Kb             2            --  \n",
      "                                        aten::unsqueeze         0.48%     136.000us        60.82%      17.174ms       1.321ms     134.000us         1.42%     186.000us      14.308us           0 b           0 b           0 b           0 b            13            --  \n",
      "                                               aten::to         0.29%      83.000us         2.27%     641.000us      45.786us     108.000us         1.14%     712.000us      50.857us           0 b           0 b      42.39 Mb           0 b            14            --  \n",
      "                                       aten::empty_like         0.41%     116.000us         1.44%     407.000us      29.071us     106.000us         1.12%     229.000us      16.357us           0 b           0 b      42.42 Mb           0 b            14            --  \n",
      "                                          aten::reshape         0.39%     111.000us         1.93%     546.000us      45.500us     104.000us         1.10%     248.000us      20.667us           0 b           0 b     397.50 Kb           0 b            12            --  \n",
      "                                           aten::arange         0.29%      83.000us         1.04%     294.000us      49.000us     101.000us         1.07%     227.000us      37.833us           0 b           0 b      18.00 Kb           0 b             6            --  \n",
      "                                        aten::transpose         0.24%      67.000us         0.50%     142.000us      17.750us      91.000us         0.96%     124.000us      15.500us           0 b           0 b           0 b           0 b             8            --  \n",
      "                                           aten::argmin         0.21%      58.000us         0.32%      91.000us      45.500us      89.000us         0.94%      98.000us      49.000us           0 b           0 b     128.00 Kb     128.00 Kb             2            --  \n",
      "                                            aten::clone         0.39%     110.000us         2.17%     613.000us      76.625us      87.000us         0.92%     508.000us      63.500us           0 b           0 b      42.23 Mb           0 b             8            --  \n",
      "                                              aten::max         0.18%      52.000us         0.39%     111.000us      55.500us      80.000us         0.85%     115.000us      57.500us           0 b           0 b       1.00 Kb           0 b             2            --  \n",
      "                                             aten::view         0.29%      83.000us         0.43%     122.000us       6.421us      79.000us         0.84%      79.000us       4.158us           0 b           0 b           0 b           0 b            19            --  \n",
      "                                          aten::resize_         0.27%      75.000us         0.31%      87.000us      12.429us      79.000us         0.84%      79.000us      11.286us           0 b           0 b       1.14 Mb       1.14 Mb             7            --  \n",
      "                                      aten::bitwise_not         0.27%      77.000us         0.36%     102.000us      25.500us      78.000us         0.83%      78.000us      19.500us           0 b           0 b      10.49 Mb      10.49 Mb             4            --  \n",
      "                                        aten::ones_like         0.18%      50.000us         0.86%     244.000us      61.000us      72.000us         0.76%     238.000us      59.500us           0 b           0 b      65.00 Kb           0 b             4            --  \n",
      "                              aten::_local_scalar_dense         0.09%      25.000us         0.27%      77.000us      19.250us      70.000us         0.74%      70.000us      17.500us           0 b           0 b           0 b           0 b             4            --  \n",
      "                                            aten::fill_         0.13%      37.000us         0.23%      65.000us      16.250us      65.000us         0.69%      65.000us      16.250us           0 b           0 b           0 b           0 b             4            --  \n",
      "                                               aten::eq         0.16%      45.000us         0.21%      60.000us      30.000us      62.000us         0.66%      62.000us      31.000us           0 b           0 b      48.00 Kb      48.00 Kb             2            --  \n",
      "                                               aten::lt         0.15%      41.000us         0.20%      57.000us      28.500us      57.000us         0.60%      57.000us      28.500us           0 b           0 b      48.00 Kb      48.00 Kb             2            --  \n",
      "                                              aten::cat         0.12%      34.000us         0.96%     271.000us      67.750us      41.000us         0.43%     275.000us      68.750us           0 b           0 b       1.13 Mb           0 b             4            --  \n",
      "                                        aten::clamp_min         0.08%      24.000us         0.14%      39.000us      19.500us      39.000us         0.41%      39.000us      19.500us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                             aten::item         0.10%      28.000us         0.44%     123.000us      30.750us      39.000us         0.41%     109.000us      27.250us           0 b           0 b           0 b           0 b             4            --  \n",
      "                                     aten::_unsafe_view         0.20%      57.000us         0.46%     131.000us      16.375us      35.000us         0.37%      54.000us       6.750us           0 b           0 b           0 b           0 b             8            --  \n",
      "                                            aten::sqrt_         0.07%      19.000us         0.12%      34.000us      17.000us      33.000us         0.35%      33.000us      16.500us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                   aten::_reshape_alias         0.11%      30.000us         0.16%      45.000us       5.625us      32.000us         0.34%      32.000us       4.000us           0 b           0 b           0 b           0 b             8            --  \n",
      "                                          aten::__and__         0.08%      22.000us         0.42%     118.000us      39.333us      30.000us         0.32%     287.000us      95.667us           0 b           0 b      10.48 Mb           0 b             3            --  \n",
      "                                       aten::contiguous         0.06%      16.000us         0.82%     231.000us     115.500us      25.000us         0.26%     230.000us     115.000us           0 b           0 b       3.00 Kb           0 b             2            --  \n",
      "                                 aten::_index_put_impl_         0.11%      30.000us         0.40%     112.000us      56.000us      25.000us         0.26%      69.000us      34.500us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                               aten::mT         0.06%      16.000us         0.20%      57.000us      28.500us      22.000us         0.23%      54.000us      27.000us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                      aten::result_type         0.04%      10.000us         0.07%      19.000us       4.750us      18.000us         0.19%      18.000us       4.500us           0 b           0 b           0 b           0 b             4            --  \n",
      "                                       aten::clamp_min_         0.05%      15.000us         0.22%      62.000us      31.000us      18.000us         0.19%      57.000us      28.500us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                       aten::index_put_         0.07%      20.000us         0.50%     141.000us      70.500us      18.000us         0.19%      87.000us      43.500us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                      aten::masked_fill         0.10%      28.000us         0.74%     210.000us     105.000us      13.000us         0.14%     466.000us     233.000us           0 b           0 b      41.84 Mb           0 b             2            --  \n",
      "                                          aten::softmax         0.04%      10.000us         0.16%      44.000us      44.000us       6.000us         0.06%     229.000us     229.000us           0 b           0 b      41.71 Mb           0 b             1            --  \n",
      "                                       cudaEventDestroy         7.06%       1.995ms         7.06%       1.995ms       1.178us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b          1694            --  \n",
      "                                        cudaEventCreate         0.38%     107.000us         0.38%     107.000us       0.123us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           870            --  \n",
      "                                        cudaEventRecord        65.60%      18.525ms        65.60%      18.525ms      21.293us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           870            --  \n",
      "                                       cudaLaunchKernel         2.40%     678.000us         2.40%     678.000us       4.291us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           158            --  \n",
      "                                               [memory]         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b    -274.50 Mb    -274.50 Mb            58            --  \n",
      "                                        cudaMemcpyAsync         0.24%      67.000us         0.24%      67.000us      16.750us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             4            --  \n",
      "                                  cudaStreamSynchronize         0.02%       6.000us         0.02%       6.000us       3.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                 cudaDeviceGetAttribute         0.00%       1.000us         0.00%       1.000us       0.083us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            12            --  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.04%      12.000us         0.04%      12.000us       0.167us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            72            --  \n",
      "                                    cudaPeekAtLastError         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           138            --  \n",
      "                                  cudaDeviceSynchronize         5.58%       1.576ms         5.58%       1.576ms       1.576ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 28.238ms\n",
      "Self CUDA time total: 9.453ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profiler.profile(profile_memory=True, use_cuda=True, with_flops=True) as prof:\n",
    "    out = clustered_attention_sorted(q, k, v, centroids, mask)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33adb9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  Total KFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::copy_         1.66%     562.000us         5.81%       1.972ms      30.812us       1.478ms         9.66%       1.478ms      23.094us           0 b           0 b           0 b           0 b            64            --  \n",
      "                                         aten::bincount         0.38%     128.000us         2.46%     836.000us     418.000us       1.342ms         8.77%       1.950ms     975.000us           0 b         -32 b       1.00 Kb      -2.00 Kb             2            --  \n",
      "                                            aten::slice         2.64%     896.000us         5.39%       1.829ms      16.627us       1.284ms         8.39%       1.722ms      15.655us           0 b           0 b           0 b           0 b           110            --  \n",
      "                                     aten::pad_sequence         2.05%     696.000us        11.63%       3.950ms       1.975ms       1.079ms         7.05%       3.948ms       1.974ms           0 b           0 b     253.50 Kb           0 b             2            --  \n",
      "                                              aten::bmm         0.27%      92.000us         0.38%     128.000us      32.000us       1.039ms         6.79%       1.039ms     259.750us           0 b           0 b      42.15 Mb      42.15 Mb             4    176729.088  \n",
      "                                       aten::as_strided         1.99%     676.000us         3.30%       1.119ms       4.886us     933.000us         6.10%     933.000us       4.074us           0 b           0 b           0 b           0 b           229            --  \n",
      "                                           aten::select         1.52%     516.000us         2.97%       1.009ms      17.102us     723.000us         4.72%     970.000us      16.441us           0 b           0 b           0 b           0 b            59            --  \n",
      "                                              aten::mul         0.67%     227.000us         1.08%     366.000us      33.273us     566.000us         3.70%     622.000us      56.545us           0 b           0 b      44.34 Mb      43.34 Mb            11     11541.184  \n",
      "                                           aten::narrow         0.98%     334.000us         3.86%       1.312ms      27.333us     486.000us         3.18%       1.261ms      26.271us           0 b           0 b           0 b           0 b            48            --  \n",
      "                                          aten::nonzero         0.76%     258.000us         6.63%       2.252ms     750.667us     419.000us         2.74%     568.000us     189.333us           0 b           0 b     704.00 Kb           0 b             3            --  \n",
      "                                            aten::empty         0.99%     336.000us         1.14%     386.000us      13.786us     329.000us         2.15%     329.000us      11.750us           0 b           0 b      42.47 Mb      42.47 Mb            28            --  \n",
      "                                     aten::tensor_split         0.61%     206.000us         3.19%       1.082ms     541.000us     326.000us         2.13%       1.079ms     539.500us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                  aten::_euclidean_dist         0.71%     242.000us         5.09%       1.730ms     865.000us     266.000us         1.74%       1.729ms     864.500us           0 b           0 b     192.00 Kb      -3.26 Mb             2            --  \n",
      "                                      aten::bitwise_and         0.17%      58.000us         0.24%      81.000us      27.000us     263.000us         1.72%     263.000us      87.667us           0 b           0 b      10.46 Mb      10.46 Mb             3            --  \n",
      "                                     aten::masked_fill_         0.10%      33.000us         0.16%      54.000us      18.000us     227.000us         1.48%     227.000us      75.667us           0 b           0 b           0 b           0 b             3            --  \n",
      "                                         aten::_softmax         0.06%      21.000us         0.09%      29.000us      29.000us     219.000us         1.43%     219.000us     219.000us           0 b           0 b      41.71 Mb      41.71 Mb             1            --  \n",
      "                                           aten::expand         0.53%     179.000us         1.02%     348.000us      17.400us     215.000us         1.40%     297.000us      14.850us           0 b           0 b           0 b           0 b            20            --  \n",
      "                                              aten::sum         0.37%     127.000us         0.69%     235.000us      47.000us     196.000us         1.28%     263.000us      52.600us           0 b           0 b      65.50 Kb      49.50 Kb             5            --  \n",
      "                                           aten::gather         0.36%     121.000us         0.59%     201.000us      50.250us     187.000us         1.22%     220.000us      55.000us           0 b           0 b     888.00 Kb     888.00 Kb             4            --  \n",
      "                                              aten::div         0.07%      23.000us         0.09%      30.000us      30.000us     186.000us         1.22%     186.000us     186.000us           0 b           0 b      41.71 Mb      41.71 Mb             1            --  \n",
      "                                           aten::arange         0.39%     133.000us         1.42%     481.000us      48.100us     179.000us         1.17%     405.000us      40.500us           0 b           0 b      66.00 Kb           0 b            10            --  \n",
      "                                              aten::pow         0.35%     119.000us         0.58%     198.000us      49.500us     167.000us         1.09%     200.000us      50.000us           0 b           0 b       1.00 Mb       1.00 Mb             4            --  \n",
      "                                           aten::matmul         0.44%     151.000us         1.95%     661.000us     165.250us     159.000us         1.04%       1.431ms     357.750us           0 b           0 b      42.15 Mb           0 b             4            --  \n",
      "                                              aten::add         0.34%     117.000us         0.48%     164.000us      27.333us     159.000us         1.04%     159.000us      26.500us           0 b           0 b     640.00 Kb     640.00 Kb             6        81.920  \n",
      "                                         aten::_to_copy         0.39%     134.000us         4.19%       1.423ms     142.300us     154.000us         1.01%     685.000us      68.500us         400 b           0 b      41.92 Mb           0 b            10            --  \n",
      "                                        aten::unsqueeze         0.41%     139.000us        48.67%      16.526ms       1.271ms     151.000us         0.99%     201.000us      15.462us           0 b           0 b           0 b           0 b            13            --  \n",
      "                                          aten::reshape         0.32%     109.000us         0.93%     316.000us      22.571us     150.000us         0.98%     283.000us      20.214us           0 b           0 b     128.00 Kb           0 b            14            --  \n",
      "                                             aten::_cat         0.25%      86.000us         0.64%     216.000us      54.000us     149.000us         0.97%     233.000us      58.250us           0 b           0 b       1.13 Mb           0 b             4            --  \n",
      "                                            aten::cdist         0.28%      95.000us         6.50%       2.206ms       1.103ms     139.000us         0.91%       2.203ms       1.101ms           0 b           0 b     192.00 Kb      -3.00 Kb             2            --  \n",
      "                                          aten::resize_         0.37%     126.000us         0.45%     152.000us      12.667us     136.000us         0.89%     136.000us      11.333us           0 b           0 b       1.85 Mb       1.85 Mb            12            --  \n",
      "                                            aten::fill_         0.22%      74.000us         0.38%     130.000us      16.250us     131.000us         0.86%     131.000us      16.375us           0 b           0 b           0 b           0 b             8            --  \n",
      "                                        aten::transpose         0.28%      96.000us         0.57%     194.000us      17.636us     122.000us         0.80%     172.000us      15.636us           0 b           0 b           0 b           0 b            11            --  \n",
      "                                             aten::view         0.30%     103.000us         0.43%     146.000us       6.636us     110.000us         0.72%     110.000us       5.000us           0 b           0 b           0 b           0 b            22            --  \n",
      "                                               aten::to         0.28%      94.000us         4.61%       1.564ms      97.750us     105.000us         0.69%     790.000us      49.375us         400 b           0 b      41.92 Mb           0 b            16            --  \n",
      "                                       aten::empty_like         0.24%      82.000us         0.84%     286.000us      28.600us      99.000us         0.65%     220.000us      22.000us           0 b           0 b      42.15 Mb           0 b            10            --  \n",
      "                                              aten::min         0.14%      49.000us         0.32%     109.000us      54.500us      94.000us         0.61%     129.000us      64.500us           0 b           0 b       1.00 Kb           0 b             2            --  \n",
      "                                            aten::index         0.19%      66.000us         6.01%       2.042ms       2.042ms      94.000us         0.61%     359.000us     359.000us           0 b           0 b      64.00 Kb    -192.00 Kb             1            --  \n",
      "                                              aten::max         0.14%      48.000us         0.39%     131.000us      65.500us      93.000us         0.61%     149.000us      74.500us           0 b           0 b       1.00 Kb           0 b             2            --  \n",
      "                                           aten::argmin         0.17%      59.000us         0.26%      88.000us      44.000us      90.000us         0.59%      98.000us      49.000us           0 b           0 b     128.00 Kb     128.00 Kb             2            --  \n",
      "                                            aten::clone         0.25%      86.000us         1.34%     454.000us      75.667us      87.000us         0.57%     523.000us      87.167us           0 b           0 b      42.09 Mb           0 b             6            --  \n",
      "                                    aten::empty_strided         0.27%      93.000us         0.34%     114.000us      11.400us      85.000us         0.56%      85.000us       8.500us         400 b         400 b      41.92 Mb      41.92 Mb            10            --  \n",
      "                                      aten::bitwise_not         0.16%      53.000us         0.22%      74.000us      24.667us      82.000us         0.54%      82.000us      27.333us           0 b           0 b      10.46 Mb      10.46 Mb             3            --  \n",
      "                                        aten::ones_like         0.14%      46.000us         0.71%     240.000us      60.000us      77.000us         0.50%     238.000us      59.500us           0 b           0 b      65.00 Kb           0 b             4            --  \n",
      "                                               aten::ne         0.17%      58.000us         0.21%      73.000us      36.500us      72.000us         0.47%      72.000us      36.000us           0 b           0 b      32.00 Kb      32.00 Kb             2            --  \n",
      "                                               aten::eq         0.13%      44.000us         0.17%      59.000us      29.500us      62.000us         0.41%      62.000us      31.000us           0 b           0 b      48.00 Kb      48.00 Kb             2            --  \n",
      "                                               aten::lt         0.12%      40.000us         0.16%      56.000us      28.000us      58.000us         0.38%      58.000us      29.000us           0 b           0 b      32.00 Kb      32.00 Kb             2            --  \n",
      "                                   aten::_reshape_alias         0.13%      45.000us         0.21%      72.000us       5.538us      56.000us         0.37%      56.000us       4.308us           0 b           0 b           0 b           0 b            13            --  \n",
      "                                             aten::set_         0.11%      39.000us         0.23%      77.000us      12.833us      54.000us         0.35%      74.000us      12.333us           0 b           0 b           0 b           0 b             6            --  \n",
      "                                              aten::cat         0.10%      34.000us         0.79%     267.000us      66.750us      39.000us         0.25%     272.000us      68.000us           0 b           0 b       1.13 Mb           0 b             4            --  \n",
      "                                             aten::full         0.08%      26.000us         0.30%     102.000us      51.000us      38.000us         0.25%     101.000us      50.500us           0 b           0 b     253.50 Kb           0 b             2            --  \n",
      "                                            aten::sqrt_         0.06%      22.000us         0.10%      34.000us      17.000us      35.000us         0.23%      35.000us      17.500us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                        aten::clamp_min         0.06%      22.000us         0.11%      38.000us      19.000us      34.000us         0.22%      34.000us      17.000us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                          aten::__and__         0.07%      23.000us         0.34%     116.000us      38.667us      34.000us         0.22%     297.000us      99.000us           0 b           0 b      10.46 Mb           0 b             3            --  \n",
      "                                      aten::masked_fill         0.13%      44.000us         0.89%     301.000us     100.333us      30.000us         0.20%     513.000us     171.000us           0 b           0 b      41.96 Mb           0 b             3            --  \n",
      "                                     aten::_unsafe_view         0.12%      40.000us         0.25%      84.000us      16.800us      28.000us         0.18%      41.000us       8.200us           0 b           0 b           0 b           0 b             5            --  \n",
      "                                           aten::cumsum         0.30%     103.000us         0.42%     141.000us      70.500us      28.000us         0.18%      32.000us      16.000us           0 b           0 b       1.00 Kb       1.00 Kb             2            --  \n",
      "                                            aten::zero_         0.05%      18.000us         0.18%      60.000us      30.000us      25.000us         0.16%      60.000us      30.000us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                        aten::expand_as         0.04%      14.000us         0.17%      58.000us      29.000us      24.000us         0.16%      57.000us      28.500us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                               aten::mT         0.04%      15.000us         0.17%      57.000us      28.500us      23.000us         0.15%      55.000us      27.500us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                       aten::contiguous         0.05%      17.000us         0.68%     230.000us     115.000us      22.000us         0.14%     228.000us     114.000us           0 b           0 b       3.00 Kb           0 b             2            --  \n",
      "                                                aten::t         0.07%      23.000us         0.26%      88.000us      29.333us      19.000us         0.12%      59.000us      19.667us           0 b           0 b           0 b           0 b             3            --  \n",
      "                                      aten::result_type         0.04%      12.000us         0.06%      20.000us       5.000us      17.000us         0.11%      17.000us       4.250us           0 b           0 b           0 b           0 b             4            --  \n",
      "                                       aten::clamp_min_         0.05%      17.000us         0.19%      63.000us      31.500us      14.000us         0.09%      48.000us      24.000us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                     aten::resolve_conj         0.02%       6.000us         0.03%      10.000us       5.000us       8.000us         0.05%       8.000us       4.000us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                      aten::resolve_neg         0.01%       5.000us         0.03%       9.000us       4.500us       8.000us         0.05%       8.000us       4.000us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                          aten::softmax         0.03%      10.000us         0.13%      43.000us      43.000us       4.000us         0.03%     223.000us     223.000us           0 b           0 b      41.71 Mb           0 b             1            --  \n",
      "                                       cudaEventDestroy         6.54%       2.220ms         6.54%       2.220ms       1.311us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b          1694            --  \n",
      "                                        cudaEventCreate         0.50%     169.000us         0.50%     169.000us       0.100us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b          1694            --  \n",
      "                                        cudaEventRecord        57.08%      19.383ms        57.08%      19.383ms      11.442us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b          1694            --  \n",
      "                                       cudaLaunchKernel         1.71%     580.000us         1.71%     580.000us       5.524us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           105            --  \n",
      "                                               [memory]         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us        -368 b        -368 b    -275.39 Mb    -275.39 Mb            75            --  \n",
      "                                 cudaDeviceGetAttribute         0.00%       1.000us         0.00%       1.000us       0.048us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            21            --  \n",
      "cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.02%       6.000us         0.02%       6.000us       0.545us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            11            --  \n",
      "                                    cudaPeekAtLastError         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            32            --  \n",
      "                                        cudaMemcpyAsync         8.65%       2.937ms         8.65%       2.937ms      48.950us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            60            --  \n",
      "                                  cudaStreamSynchronize         0.04%      15.000us         0.04%      15.000us       1.667us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             9            --  \n",
      "                                         cudaMemGetInfo         0.21%      73.000us         0.21%      73.000us      36.500us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             2            --  \n",
      "                                  cudaDeviceSynchronize         0.02%       6.000us         0.02%       6.000us       6.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 33.956ms\n",
      "Self CUDA time total: 15.307ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profiler.profile(profile_memory=True, use_cuda=True, with_flops=True) as prof:\n",
    "    out = clustered_attention_vectorized(q, k, v, centroids, mask)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45736468",
   "metadata": {},
   "source": [
    "## Going the extra mile: blockfied attention\n",
    "\n",
    "BigBird's self-attention relies on two simplifications to get speed improvements over vanilla self-attention:\n",
    "\n",
    "1. **Local + global + random connections:** Only attend to pre-determined elements, leading to a fixed pattern in the attention matrix\n",
    "2. **Blocks:** Group contiguous tokens into chunks, leading to a blockfied pattern in the attention matrix\n",
    "\n",
    "While the first point is important for attending to relevant elements, the second point is crucial for optimizing runtime. How can we introduce blocks in our implementation? Fortunetally, we can rely (again) on broadcasting to implement a routine that blockfy inputs and another routine to unblockfy the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c46185f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 1024, 16]), torch.Size([2, 4]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, q.shape[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "92bfa301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def blockfy(input, block_size=2):\n",
    "    seq_len = input.shape[-2]\n",
    "    n_blocks = math.ceil(seq_len / float(block_size))\n",
    "    # pad so that seq_len becomes divisible by block_size\n",
    "    # (batch, heads, seq_len, hdim) -> (batch, heads, seq_len + seq_len % block_size, hdim)\n",
    "    input_pad = torch.nn.functional.pad(input, (0, 0, 0, block_size - seq_len % block_size))\n",
    "    # reshape to get contiguous chunks\n",
    "    # (batch, heads, n, hdim) -> (batch, heads, n_blocks, block_size * hdim)\n",
    "    return input_pad.view(*input_pad.shape[:-2], n_blocks, -1)\n",
    "\n",
    "\n",
    "def unblockfy(output, seq_len, block_size=2):\n",
    "    n_blocks = output.shape[-2]\n",
    "    # (batch, heads, n_blocks, block_size * hdim) -> (batch, heads, n_blocks * block_size, hdim)\n",
    "    output_pad = output.view(batch_size, num_heads, n_blocks * block_size, -1)\n",
    "    # cut pad out\n",
    "    return output_pad[:, :, :seq_len]\n",
    "\n",
    "\n",
    "def unblockfy_attn(att_dist, seq_len, block_size=2, pad_mask=None, causal_mask=None):\n",
    "    # (batch, heads, n_blocks, n_blocks) -> (batch, heads, n_blocks * block_size, n_blocks * block_size)\n",
    "    att = att_dist.repeat_interleave(block_size, dim=-1).repeat_interleave(block_size, dim=-2)\n",
    "    # mask out padding and \"future\" positions\n",
    "    if pad_mask is not None:\n",
    "        # (batch, seq_len) -> (batch, n_blocks * block_size, n_blocks * block_size)\n",
    "        pairwise_mask = pad_mask.unsqueeze(-1) & pad_mask.unsqueeze(1)\n",
    "        # add head dimension\n",
    "        pairwise_mask = pairwise_mask.unsqueeze(1)\n",
    "        if causal_mask is not None:\n",
    "            # add elements of the triu to the mask\n",
    "            pairwise_mask = pairwise_mask & causal_mask.unsqueeze(0).unsqueeze(1)\n",
    "        # mask out \n",
    "        att = att.masked_fill(~pairwise_mask, 0)\n",
    "    # note that att is not a distribution anymore\n",
    "    return att[..., :seq_len, :seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b79ae62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 1024, 16])\n",
      "torch.Size([2, 4, 342, 48])\n",
      "torch.Size([2, 4, 1024, 16])\n"
     ]
    }
   ],
   "source": [
    "print(q.shape)\n",
    "print(blockfy(q, block_size=3).shape)\n",
    "print(unblockfy(blockfy(q, block_size=3), seq_len=q.shape[-2], block_size=3).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9a5f2d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 1024, 1024])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_dist = torch.randn(2, 4, 342, 342)\n",
    "unblockfy_attn(att_dist, seq_len=q.shape[-2], block_size=3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e4053a",
   "metadata": {},
   "source": [
    "That is! We can simply call our attention module with:\n",
    "\n",
    "```python\n",
    "q_block = blockfy(q)\n",
    "k_block = blockfy(k)\n",
    "v_block = blockfy(v)\n",
    "mask_block = blockfy(mask.unsqueeze(-1), block_size=3).any(-1)\n",
    "```\n",
    "\n",
    "And then in the end we can reshape the output back to the original sequence length:\n",
    "```python\n",
    "output = unblockfy(output, seq_len=seq_len, block_size=3)\n",
    "# note that mask and causal_mask should be padded to\n",
    "# have a length of n_blocks * block_size\n",
    "att_dist = unblockfy_attn(att_dist, mask, causal_mask)  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823406b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Computing attention statistics\n",
    "\n",
    "Given that we are working with batches and heads, how can we compute independent statistics for attention maps? For example, we could be interested in computing the ammount of sparsity, or the recall when compared with a gold attention pattern. The traditional (and safe) way of doing this would envolve flattening tensors and calling a standard method from a well-tested library. However, we would be ignoring all the power that PyTorch brings to us. In fact, in cases like this is where PyTorch's broadcasting shines.\n",
    "\n",
    "To see the difference, let's implement a traditional version and some PyTorch versions of two statistics: **sparsity** and **recall**.\n",
    "\n",
    "Quick setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "c7bf471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "sequence_length = 9\n",
    "num_heads = 3\n",
    "head_size = 2\n",
    "hidden_size = num_heads * head_size\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "att_dist = torch.randn(batch_size, num_heads, sequence_length, sequence_length, device=device).softmax(dim=-1)\n",
    "gold_att_dist = torch.randint(0, 2, size=att_dist.shape, device=device)\n",
    "mask = torch.tensor([5, 8]).unsqueeze(-1) >= torch.arange(sequence_length).unsqueeze(0).expand(batch_size, -1)\n",
    "mask = mask.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b3d60",
   "metadata": {},
   "source": [
    "Since softmax always produce nonzero probabilities, let's set some values to zero arbitraryly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "3ee6f526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.4710, 0.0000, 0.0000, 0.1022, 0.0000, 0.0000],\n",
       "        [0.2364, 0.0000, 0.2462, 0.0000, 0.0000, 0.1265, 0.1126, 0.1362, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1799, 0.0000, 0.0000, 0.2459, 0.0000, 0.2417, 0.0000],\n",
       "        [0.1417, 0.0000, 0.0000, 0.0000, 0.0000, 0.1482, 0.1141, 0.1394, 0.3346],\n",
       "        [0.0000, 0.2016, 0.2307, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2868],\n",
       "        [0.1814, 0.0000, 0.0000, 0.0000, 0.0000, 0.3844, 0.0000, 0.0000, 0.1693],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3962, 0.0000, 0.2924, 0.0000, 0.0000],\n",
       "        [0.0000, 0.2786, 0.0000, 0.1485, 0.0000, 0.1361, 0.0000, 0.0000, 0.2319],\n",
       "        [0.0000, 0.2886, 0.1361, 0.0000, 0.1505, 0.0000, 0.2319, 0.0000, 0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_dist = att_dist.masked_fill(att_dist < 0.1, 0)\n",
    "att_dist[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7187a3d",
   "metadata": {},
   "source": [
    "One thing to keep in mind is that Transformers' self-attention are masked for keys (last dimension) but not for queries (second last dimension). In practice this behavior does not impact the padding positions are simply ignored for the final task. However, when computing attention statistics we have to take these padded positions into account. \n",
    "\n",
    "We will start with raw python to simplify things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "74af96cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sparsity_vanilla(att_dist, mask):\n",
    "    batch_size, num_heads, _, _ = att_dist.shape\n",
    "    p = 0  # positive count\n",
    "    n = 0  # total count\n",
    "    for i in range(batch_size):\n",
    "        valid_seq_len = mask[i].sum().item()  \n",
    "        n += num_heads * valid_seq_len ** 2\n",
    "        for h in range(num_heads):\n",
    "            p += sum([int(att_dist[i, h, k1, k2].item() > 0) \n",
    "                      for k1 in range(valid_seq_len) \n",
    "                      for k2 in range(valid_seq_len)])\n",
    "    return 1 - p / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ccd9393b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 9], device='cuda:0')"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "f700c307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5982905982905983"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_sparsity_vanilla(att_dist, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b59fb",
   "metadata": {},
   "source": [
    "Now with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "3c15547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sparsity_vectorized(att_dist, mask):\n",
    "    pairwise_mask = mask[:, None, :, None] & mask[:, None, None, :]\n",
    "    p = (att_dist > 0).masked_fill(~pairwise_mask, False).sum().item()\n",
    "    n = num_heads * pairwise_mask.sum().item()\n",
    "    return 1 - p / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "1a3e2a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5982905982905983"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_sparsity_vectorized(att_dist, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e2dab8",
   "metadata": {},
   "source": [
    "Simple and efficient!\n",
    "\n",
    "If we were using a encoder-decoder transformer we would also need to account for causal masks (future position masking). This could be implementead easily as follows:\n",
    "\n",
    "```python\n",
    "pairwise_mask = pairwise_mask & causal_mask[:, None, :, :]\n",
    "```\n",
    "\n",
    "Now let's turn to the other statistic: recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "46cde6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_recall_vanilla(gold_att_dist, pred_att_dist, mask):\n",
    "    from sklearn.metrics import recall_score\n",
    "    batch_size, num_heads, _, _ = pred_att_dist.shape\n",
    "    recalls = torch.zeros(batch_size, num_heads)\n",
    "    for i in range(batch_size):\n",
    "        valid_seq_len = mask[i].sum().item()  \n",
    "        for h in range(num_heads):\n",
    "            g = (gold_att_dist[i, h, :valid_seq_len, :valid_seq_len] > 0).long().flatten().tolist()\n",
    "            p = (pred_att_dist[i, h, :valid_seq_len, :valid_seq_len] > 0).long().flatten().tolist()\n",
    "            recalls[i, h] = recall_score(g, p)\n",
    "    return recalls.mean()  # macro-averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "6ff62d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4218)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_recall_vanilla(gold_att_dist, att_dist, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "eb0156f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_recall_vectorized(gold_att_dist, pred_att_dist, mask):\n",
    "    pairwise_mask = mask[:, None, :, None] & mask[:, None, None, :]\n",
    "    g = (gold_att_dist > 0).masked_fill(~pairwise_mask, False)\n",
    "    p = (pred_att_dist > 0).masked_fill(~pairwise_mask, False)\n",
    "    matches_per_head_and_batch = (p & g).sum(-1).sum(-1).float() / g.sum(-1).sum(-1).float()\n",
    "    return matches_per_head_and_batch.mean()  # macro-averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "0850e180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4218, device='cuda:0')"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_recall_vectorized(gold_att_dist, att_dist, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2551b2",
   "metadata": {},
   "source": [
    "That is it! To finalize, let's write one more statistic function: a method that returns the portion of fully recovered predictions, i.e., the portion of predicted attention distributions with 100% recall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "ddea4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact_fraction(gold_att_dist, pred_att_dist, mask):\n",
    "    pairwise_mask = mask[:, None, :, None] & mask[:, None, None, :]\n",
    "    g = (gold_att_dist > 0).masked_fill(~pairwise_mask, False)\n",
    "    p = (pred_att_dist > 0).masked_fill(~pairwise_mask, False)\n",
    "    matches = p & g\n",
    "    matches_per_query = matches.sum(-1).float()\n",
    "    total_per_query = g.sum(-1).float()\n",
    "    # might get nans due to zero division\n",
    "    recall_per_query = matches_per_query / total_per_query\n",
    "    exact_per_query = recall_per_query == 1.0\n",
    "    # filter nans out\n",
    "    valid_exact_per_query = exact_per_query.masked_fill(~mask[:, None, :], False)\n",
    "    lengths = mask.sum(-1).unsqueeze(-1).float()\n",
    "    exact_per_head_and_batch = valid_exact_per_query.sum(-1) / lengths\n",
    "    return exact_per_head_and_batch.mean()  # macro-averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "1e3ad95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0741, device='cuda:0')"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_exact_fraction(gold_att_dist, att_dist, mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
