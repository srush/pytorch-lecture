{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation Efficiency with Numpy, PyTorch, and JIT\n",
    "\n",
    "This notebooks illustrates the computational efficiency of running linear algebra with the proper tools - such as numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_times(labels, times):\n",
    "    x = list(range(len(times)))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.grid(alpha=0.5, ls='--', which='both')\n",
    "    ax.bar(x, times, log=True)\n",
    "    ax.set_xticks(x, labels)\n",
    "    ax.set_axisbelow(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute an array dot product in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_dot_product(v1, v2):\n",
    "    result = 0\n",
    "    for v1_i, v2_i in zip(v1, v2):\n",
    "        result += v1_i * v2_i\n",
    "    return result\n",
    "\n",
    "v1 = list(range(100))\n",
    "v2 = [1]*100\n",
    "\n",
    "print(\"v1 = {}\".format(v1))\n",
    "print(\"v2 = {}\\n\".format(v2))\n",
    "\n",
    "print(\"v1 dot v2: {}\".format(array_dot_product(v1, v2)))\n",
    "print(\"1+2+...+99:\", 99*100/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, it works, but how long does it take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit array_dot_product(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enters numpy\n",
    "\n",
    "Now let's try with numpy, which uses a C backend optimized for mathematical operations, alleviating the Python overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_np = np.arange(100)\n",
    "v2_np = np.ones(100)\n",
    "print(\"v1 dot v2: {}\".format(v1_np.dot(v2_np)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, aligned with our raw Python version. Now let's check the running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit v1_np.dot(v2_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already se the difference. Numpy was roughly 6x faster than raw PyTorch for a very small array. New let's check with matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_mul(m1, m2):\n",
    "    num_rows = len(m1)\n",
    "    num_columns = len(m2[0])\n",
    "    internal_dim = len(m1[0])\n",
    "    result = []\n",
    "    for i in range(num_rows):\n",
    "        new_row = []\n",
    "        for j in range(num_columns):\n",
    "            total = 0\n",
    "            for k in range(internal_dim):\n",
    "                total += m1[i][k] * m2[k][j]\n",
    "            new_row.append(total)\n",
    "        result.append(new_row)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_np = np.random.randn(100, 200)\n",
    "m2_np = np.random.randn(200, 100)\n",
    "m1_list = m1_np.tolist()\n",
    "m2_list = m2_np.tolist()\n",
    "\n",
    "result_raw = matrix_mul(m1_list, m2_list)\n",
    "result_np = m1_np.dot(m2_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = np.abs(result_raw - result_np).sum()\n",
    "print('{} up to {}'.format(np.allclose(result_raw, result_np), eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. Now lets time it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_raw = %timeit -o matrix_mul(m1_list, m2_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_np = %timeit -o m1_np.dot(m2_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_ratio = time_raw.average / time_np.average\n",
    "print('Numpy is ~{:.0f}x faster than standard python'.format(time_ratio))\n",
    "print('Something the runs in 1h in numpy would need to run for {:.0f} days in raw python'.format(time_ratio / 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_times(['python', 'numpy'], [time_raw.average, time_np.average])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enters PyTorch\n",
    "\n",
    "Now let's try with PyTorch. Note that PyTorch also uses a C-backend to implement linear algebra methods. However, it also has the power to run those operation on GPUs. Let's try both variants and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_pt = torch.from_numpy(m1_np)\n",
    "m2_pt = torch.from_numpy(m2_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pt = %timeit -o m1_pt @ m2_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_times(['python', 'numpy', 'pytorch'], \n",
    "           [time_raw.average, time_np.average, time_pt.average])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems about the same... Now let's try to use a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_pt = m1_pt.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "m2_pt = m2_pt.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "time_pt_gpu = %timeit -o m1_pt @ m2_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_times(['numpy', 'pytorch (cpu)', 'pytorch (gpu)'], \n",
    "           [time_np.average, time_pt.average, time_pt_gpu.average])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enters JIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we have an even more complicated function that contains control flows (if-else statements). To handle that, we have to rely on the Python interpreter, which is slow. To circumvent that, we can \"compile\" our function/module into a fixed intermediate-level code representation. \n",
    "\n",
    "https://pytorch.org/docs/stable/jit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def jit_mm(m1, m2):\n",
    "    return m1 @ m2\n",
    "\n",
    "time_pt_jit = %timeit -o jit_mm(m1_pt, m2_pt)\n",
    "\n",
    "plot_times(['numpy', 'pt (cpu)', 'pt (gpu)', 'pt (gpu+jit)'], \n",
    "           [time_np.average, time_pt.average, time_pt_gpu.average, time_pt_jit.average])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more optimizations, check this blog post by Horace He:\n",
    "[Making Deep Learning Go Brrrr From First Principles](https://horace.io/brrr_intro.html)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
